output_dir: output

gn_model:
  _target_: src.models.mnist_module.MNISTLitModule

  optimizer:
    _target_: torch.optim.Adam
    _partial_: true
    lr: 0.001
    weight_decay: 0.0

  scheduler:
    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    _partial_: true
    mode: min
    factor: 0.1
    patience: 10

  net:
    _target_: src.models.components.gn_model.ConvNet
    norm_layer: 
      _target_: torch.nn.GroupNorm
      _partial_: true

  use_l1: false
  regularizer:
    _target_: src.models.components.l1_reg.L1RegularizedConv2D
    weight_decay: 0.0001
    _partial_: true

bn_model:
  _target_: src.models.mnist_module.MNISTLitModule

  optimizer:
    _target_: torch.optim.Adam
    _partial_: true
    lr: 0.001
    weight_decay: 0.0

  scheduler:
    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    _partial_: true
    mode: min
    factor: 0.1
    patience: 10

  net:
    _target_: src.models.components.bn_model.ConvNet
    norm_layer: 
      _target_: torch.nn.BatchNorm2d
      _partial_: true

  use_l1: True
  regularizer:
    _target_: src.models.components.l1_reg.L1RegularizedConv2D
    weight_decay: 0.0001
    _partial_: true

ln_model:
  _target_: src.models.mnist_module.MNISTLitModule

  optimizer:
    _target_: torch.optim.Adam
    _partial_: true
    lr: 0.001
    weight_decay: 0.0

  scheduler:
    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    _partial_: true
    mode: min
    factor: 0.1
    patience: 10

  net:
    _target_: src.models.components.ln_model.ConvNet
    norm_layer: 
      _target_: torch.nn.LayerNorm
      _partial_: true

  use_l1: False
  regularizer:
    _target_: src.models.components.l1_reg.L1RegularizedConv2D
    weight_decay: 0.0001
    _partial_: true

datamodule:
  _target_: src.datamodules.mnist_datamodule.MNISTDataModule
  data_dir: data
  batch_size: 1000
  train_val_test_split: [55_000, 5_000, 10_000]
  num_workers: 0
  pin_memory: False

trainer:
  _target_: pytorch_lightning.Trainer

  default_root_dir: ${output_dir}

  min_epochs: 1 # prevents early stopping
  max_epochs: 5

  accelerator: gpu
  devices: 1

  # mixed precision for extra speed-up
  # precision: 16

  # perform a validation loop every N training epochs
  check_val_every_n_epoch: 1

  # set True to to ensure deterministic results
  # makes training slower but gives more reproducibility than just setting seeds
  deterministic: False

csv:
  _target_: pytorch_lightning.loggers.csv_logs.CSVLogger
  save_dir: "${output_dir}"
  name: "csv/"
  prefix: ""


model_checkpoint:
  _target_: pytorch_lightning.callbacks.ModelCheckpoint
  dirpath: data/checkpoints # directory to save the model file
  filename:  "epoch_{epoch:03d}" # checkpoint filename
  monitor: "val/acc" # name of the logged metric which determines when model is improving
  verbose: False # verbosity mode
  save_last: True # additionally always save an exact copy of the last checkpoint to a file last.ckpt
  save_top_k: 1 # save k best models (determined by above metric)
  mode: "max" # "max" means higher metric value is better, can be also "min"
  auto_insert_metric_name: True # when True, the checkpoints filenames will contain the metric name
  save_weights_only: False # if True, then only the modelâ€™s weights will be saved
  every_n_train_steps: null # number of training steps between checkpoints
  train_time_interval: null # checkpoints are monitored at the specified time interval
  every_n_epochs: null # number of epochs between checkpoints
  save_on_train_epoch_end: null # whether to run checkpointing at the end of the training epoch or the end of validation


early_stopping:
  _target_: pytorch_lightning.callbacks.EarlyStopping
  monitor: "val/acc" # quantity to be monitored, must be specified !!!
  min_delta: 0. # minimum change in the monitored quantity to qualify as an improvement
  patience: 10 # number of checks with no improvement after which training will be stopped
  verbose: False # verbosity mode
  mode: "max" # "max" means higher metric value is better, can be also "min"
  strict: True # whether to crash the training if monitor is not found in the validation metrics
  check_finite: True # when set True, stops training when the monitor becomes NaN or infinite
  stopping_threshold: null # stop training immediately once the monitored quantity reaches this threshold
  divergence_threshold: null # stop training as soon as the monitored quantity becomes worse than this threshold
  check_on_train_epoch_end: null # whether to run early stopping at the end of the training epoch
  # log_rank_zero_only: False  # this keyword argument isn't available in stable version